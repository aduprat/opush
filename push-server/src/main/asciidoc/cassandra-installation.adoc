== Cassandra Setup

=== Requirements

  * *Cassandra* 2.1 or 2.0 (version >= 2.0.6)
  * At least 2GiB of RAM
  * The *Cassandra* cluster must be installed (guides for
http://www.datastax.com/documentation/cassandra/2.1/cassandra/install/installDeb_t.html[Debian]
and
http://www.datastax.com/documentation/cassandra/2.1/cassandra/install/installRHEL_t.html[RHEL])
and http://www.datastax.com/documentation/cassandra/2.1/cassandra/initialize/initializeSingleDS.html[configured]

WARNING: Despite what datastax documentation says, *Cassandra* works well on OpenJDK 7

=== Supported Cassandra Configurations

[NOTE]
====
A single *Cassandra* server is enough to make Opush work but you will not have
every benefit of a *Cassandra* cluster. If you want the strongest architecture
you will have to deploy at least 3 nodes to have durability and fault-tolerance.
====

==== Three Nodes (recommended)

Official instructions to setup a cluster can be found http://www.datastax.com/documentation/cassandra/2.1/cassandra/initialize/initializeSingleDS.html[here]. +
You can find below an example for a three nodes cluster, only +cluster_name+, +seeds+ and +rpc_address+ fields have to be modified to get your cluster working. We will not cover advanced configuration in this documentation.

Assuming that you have three servers with IPs +192.168.56.1+, +192.168.56.2+ & +192.168.56.3+ and you want only one +seed+.

.+cassandra.yaml+ example on +192.168.56.1+
****
----
...
cluster_name: 'MyOpushCluster'
...
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: "192.168.56.1"
...
rpc_address: 192.168.56.1
....
----
****
.+cassandra.yaml+ example on +192.168.56.2+
****
----
...
cluster_name: 'MyOpushCluster'
...
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: "192.168.56.1"
...
rpc_address: 192.168.56.2
....
----
****
.+cassandra.yaml+ example on +192.168.56.3+
****
----
...
cluster_name: 'MyOpushCluster'
...
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: "192.168.56.1"
...
rpc_address: 192.168.56.3
....
----
****


==== Single Node

For small configurations you may want to install *Cassandra* on a single node,
to understand what is going on in this situation read this http://planetcassandra.org/blog/post/cassandra-faq-can-i-start-with-a-single-node/[article].

Assuming that your *Cassandra* server has the IP +192.168.56.1+, modify your +cassandra.yaml+ like this:

****
----
...
commitlog_sync_period_in_ms: 2000 # suggestion only, can be customized
...
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: "127.0.0.1"
...
rpc_address: 192.168.56.1
....
----
****

=== Authentication Configuration

==== Configure the Authentication Backend

Authentication in *Cassandra* must be delegated to the password authenticator backend, 
you have to edit the +cassandra.yaml+ file and check the authenticator backend:

[source]
----
authenticator: PasswordAuthenticator
----
Make the change on each node and restart them, that will create the +system_auth+ keyspace on the cluster.

==== Configure the System Users Replication

[WARNING]
====
.We advise to replicate the +system_auth+ keyspace on every cluster's node. +
*Cassandra* uses the +system_auth+ keyspace for storing security authentication and authorization information.
So if you only have one node, you're already done and you can skip this step.
====

Connect your *Cassandra* cluster from any node, and change the keyspace configuration *on this node only*. +
Replace in the following command +CLUSTER_NODE_COUNT+ by the cluster node count.

[source]
----
$ cqlsh -u cassandra -p cassandra NODE_RPC_ADDRESS
cqlsh> ALTER KEYSPACE system_auth WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': CLUSTER_NODE_COUNT};
----

You can check the new keyspace configuration:

[source]
----
cqlsh> DESCRIBE KEYSPACE system_auth ;
----

Now you have to propagate this change *on every node* with the +nodetool+ binary.
If your cluster is not in a production context you can run it on every node at the same time.
Be aware that this command can be long to terminate.

[source]
----
$ nodetool repair system_auth
----

=== Preparing for Opush
First of all, you have to create a keyspace, this must be done in the +cqlsh+ shell with *Cassandra* administrator:

[source]
----
$ cqlsh -u cassandra -p cassandra NODE_RPC_ADDRESS
---- 

A keyspace is created with a data replication configuration, to understand it please read this http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureDataDistributeReplication_c.html[page]. +
Create it with the following command in the +cqlsh+ shell:

[source]
----
cqlsh> CREATE KEYSPACE opush WITH REPLICATION = {'class' : 'CHOSEN_STRATEGY', 'replication_factor': CHOSEN_FACTOR};
----
Where:

  * +opush+ is the keyspace name (Opush 3.0.0 requires a keyspace named "opush", see http://ci-obm.linagora.com/jira/browse/OP-35[OP-35])
  * +CHOSEN_STRATEGY+ use +SimpleStrategy+ unless your cluster is deployed across multiple data centers
  * +CHOSEN_FACTOR+ must be +1+ for a single node installation, we suggest +3+ for a three nodes cluster
  
Finally, still in the +cqlsh+ shell, create a *Cassandra* user for Opush:
[source]
----
cqlsh> USE opush;
cqlsh> CREATE USER opush_user WITH PASSWORD 'opush_password' SUPERUSER;
----
Where:

  * +opush_user+ is the *Cassandra* user for Opush
  * +opush_password+ is the password for this user

[NOTE]
====
.You have more than one node ? +
Keyspace as user creation need to be done on one node, such changes affect the whole cluster
====
